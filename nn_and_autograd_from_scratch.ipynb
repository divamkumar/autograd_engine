{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f1edb4-daa9-41e3-8d73-221f08205539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b492a87b-b95b-4d6a-9650-c9c993a00b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroDimTensor:\n",
    "    def __init__(self, val, _children=(), _op=\"\", label=\"\"):\n",
    "        self.val = float(val)\n",
    "        self.grad = 0.0\n",
    "        self._op = _op # '_op' for consistency with Pytorch\n",
    "        self.label = label\n",
    "        self._children = set(_children)\n",
    "        self._backward = lambda: None # this i \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ZeroDimTensor(label='{self.label}', val={self.val}, grad={self.grad}, op='{self._op}')\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, ZeroDimTensor) else ZeroDimTensor(other)\n",
    "        return_val = ZeroDimTensor(self.val + other.val, (self, other), \"+\")\n",
    "\n",
    "        def _backward_pass_add():\n",
    "            # Gradients accumulate, so use += instead of = for all backward pass methods\n",
    "            self.grad += return_val.grad\n",
    "            other.grad += return_val.grad\n",
    "            \n",
    "        return_val._backward = _backward_pass_add # Assign to _backward\n",
    "        \n",
    "        return return_val\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, ZeroDimTensor) else ZeroDimTensor(other)\n",
    "        return_val = ZeroDimTensor(self.val * other.val, (self, other), \"*\")\n",
    "        \n",
    "        def _backward_pass_mul():\n",
    "            self.grad += other.val * return_val.grad\n",
    "            other.grad += self.val * return_val.grad\n",
    "            \n",
    "        return_val._backward = _backward_pass_mul # Assign to _backward\n",
    "        \n",
    "        return return_val\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, ZeroDimTensor) else ZeroDimTensor(other)\n",
    "        return self + (-other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"Power must be a scalar\"\n",
    "        return_val = ZeroDimTensor(self.val**other, (self,), f'**{other}')\n",
    "        \n",
    "        def _backward_pass_pow():\n",
    "            self.grad += return_val.grad * (other * (self.val**(other-1)))\n",
    "        return_val._backward = _backward_pass_pow\n",
    "        return return_val\n",
    "\n",
    "    def tanh(self):\n",
    "        return_val = ZeroDimTensor(math.tanh(self.val), (self,), \"tanh\")\n",
    "        \n",
    "        def _backward_pass_tanh():\n",
    "            # Correct local derivative for tanh\n",
    "            self.grad += (1 - return_val.val**2) * return_val.grad \n",
    "            \n",
    "        return_val._backward = _backward_pass_tanh # Assign to _backward\n",
    "        \n",
    "        return return_val\n",
    "\n",
    "    def backward(self): # Implements the backward pass (like PyTorch API)\n",
    "        # Zero out all gradients in the graph before starting\n",
    "        nodes_to_zero = set()\n",
    "        stack = [self]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            if node not in nodes_to_zero:\n",
    "                nodes_to_zero.add(node)\n",
    "                for child in node._children:\n",
    "                    stack.append(child)\n",
    "        \n",
    "        for node in nodes_to_zero:\n",
    "            node.grad = 0.0\n",
    "\n",
    "        topological_ordering = []\n",
    "        visited = set()\n",
    "        \n",
    "        def topological_sort(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._children:\n",
    "                    topological_sort(child)\n",
    "                topological_ordering.append(node)\n",
    "\n",
    "        self.grad = 1.0 # Derivative of the root node (loss) is always 1\n",
    "        topological_sort(self)\n",
    "\n",
    "        for tensor in topological_ordering[::-1]: # Iterate in reverse topological order\n",
    "            tensor._backward() # Call the stored backward function\n",
    "\n",
    "        # For debugging: print tensors and their gradients after backward pass\n",
    "        print(\"\\n--- Tensors and Gradients (after backward pass) ---\")\n",
    "        for tensor in topological_ordering:\n",
    "            print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b70971-1c99-4020-91e0-c349490a5faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tensors and Gradients (after backward pass) ---\n",
      "ZeroDimTensor(label='x2', val=0.0, grad=0.4999813233768232, op='')\n",
      "ZeroDimTensor(label='w2', val=1.0, grad=0.0, op='')\n",
      "ZeroDimTensor(label='w2x2', val=0.0, grad=0.4999813233768232, op='*')\n",
      "ZeroDimTensor(label='w1', val=-3.0, grad=0.9999626467536464, op='')\n",
      "ZeroDimTensor(label='x1', val=2.0, grad=-1.4999439701304698, op='')\n",
      "ZeroDimTensor(label='w1x1', val=-6.0, grad=0.4999813233768232, op='*')\n",
      "ZeroDimTensor(label='w1x1_plus_w2x2', val=-6.0, grad=0.4999813233768232, op='+')\n",
      "ZeroDimTensor(label='b', val=6.8814, grad=0.4999813233768232, op='')\n",
      "ZeroDimTensor(label='n', val=0.8814000000000002, grad=0.4999813233768232, op='+')\n",
      "ZeroDimTensor(label='o', val=0.7071199874301226, grad=1.0, op='tanh')\n"
     ]
    }
   ],
   "source": [
    "x1 = ZeroDimTensor(2.0, (), \"\", \"x1\")\n",
    "w1 = ZeroDimTensor(-3.0, (), \"\", \"w1\")\n",
    "\n",
    "x2 = ZeroDimTensor(0.0, (), \"\", \"x2\")\n",
    "w2 = ZeroDimTensor(1.0, (), \"\", \"w2\")\n",
    "\n",
    "b = ZeroDimTensor(6.8814, (), \"\", \"b\")\n",
    "\n",
    "w1x1 = w1 * x1\n",
    "w1x1.label = \"w1x1\"\n",
    "\n",
    "w2x2 = w2 * x2\n",
    "w2x2.label = \"w2x2\"\n",
    "\n",
    "w1x1_plus_w2x2 = w1x1 + w2x2\n",
    "w1x1_plus_w2x2.label = \"w1x1_plus_w2x2\"\n",
    "\n",
    "n = w1x1_plus_w2x2 + b\n",
    "n.label = \"n\"\n",
    "\n",
    "o = n.tanh()\n",
    "o.label = \"o\"\n",
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88cd3800-4a45-432f-8b7b-823d2541ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, num_inputs):\n",
    "        # Initialize weights as a list of ZeroDimTensor objects\n",
    "        self.weights = [ZeroDimTensor(random.uniform(-1,1), label=f\"w{i}\") for i in range(num_inputs)] # random weights with labels\n",
    "        # Initialize bias as a ZeroDimTensor object\n",
    "        self.bias = ZeroDimTensor(random.uniform(-1,1), label=\"b\")\n",
    "\n",
    "    def __call__(self, x_inputs):\n",
    "        raw_activation = ZeroDimTensor(0.0, label=\"raw_act\") \n",
    "        \n",
    "        # Calculate w_i * x_i + b\n",
    "        for i in range(len(x_inputs)):\n",
    "            # Ensure x_inputs[i] is a ZeroDimTensor. If it's a float, __mul__ will convert it.\n",
    "            # But passing ZeroDimTensor explicitly is safer for graph consistency.\n",
    "            raw_activation += x_inputs[i] * self.weights[i]\n",
    "        \n",
    "        raw_activation += self.bias \n",
    "\n",
    "        # Apply the activation function (tanh)\n",
    "        return raw_activation.tanh() # This proves that perceptrons and neural nets are just mathematical expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c9d48f-55cf-4587-b49b-6f88bc3bb1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZeroDimTensor(label='', val=-0.9999999979429093, grad=0.0, op='tanh')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = Perceptron(2)\n",
    "x = [9.0, 10.0]\n",
    "neuron(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff25e4c-45f7-4f2e-8a59-6b5034366c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        # Initialize the layer of perceptrons with random weights and biases\n",
    "        self.perceptrons = [Perceptron(num_inputs) for _ in range(num_outputs)] # Renamed for clarity\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Do a forward pass, passing in x (inputs) for each perceptron in the layer\n",
    "        # The output is a list of ZeroDimTensor objects\n",
    "        return [perceptron(x) for perceptron in self.perceptrons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759d5da2-c64d-47e6-8cb1-49333a1ba7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ZeroDimTensor(label='', val=0.9999919634512422, grad=0.0, op='tanh'),\n",
       " ZeroDimTensor(label='', val=-0.9999999574766677, grad=0.0, op='tanh'),\n",
       " ZeroDimTensor(label='', val=0.9999999928429436, grad=0.0, op='tanh')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Layer(2, 3) # 2 inputs per perceptron, and there is a layer of 3 perceptrons\n",
    "x = [9.0, 10.0]\n",
    "layer(x) # output the forward pass for 3 perceptrons each with input values x, random weights, and a random bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92cd27b-cf20-41fa-a14b-d3724a40f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, num_inputs, num_outputs_per_layer):\n",
    "        # sizes = [input_layer_size, hidden_layer1_size, ..., output_layer_size]\n",
    "        sizes = [num_inputs] + num_outputs_per_layer \n",
    "\n",
    "        # Create layers such that the number of outputs for each layer correctly matches the number of inputs for the next one\n",
    "        self.layers = [Layer(sizes[i - 1], sizes[i]) for i in range(1, len(sizes))]\n",
    "\n",
    "    def __call__(self, x_inputs):\n",
    "        # Pass the input through each layer sequentially\n",
    "        for layer in self.layers:\n",
    "            x_inputs = layer(x_inputs) # The output of one layer becomes the input for the next\n",
    "        return x_inputs # The final output of the network\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e4f7bbf-08fd-4dde-9f90-e19e069d39a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ZeroDimTensor(label='', val=-0.9219678047939753, grad=0.0, op='tanh')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1.0, 2.0, -3.0]\n",
    "nn = MultiLayerPerceptron(3, [4, 4, 1])\n",
    "nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c581fbe-90ff-486a-8461-2e6ef477e092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ZeroDimTensor(label='', val=0.44183136181958993, grad=0.0, op='tanh')],\n",
       " [ZeroDimTensor(label='', val=0.8560231358110179, grad=0.0, op='tanh')],\n",
       " [ZeroDimTensor(label='', val=0.8073855243948576, grad=0.0, op='tanh')],\n",
       " [ZeroDimTensor(label='', val=0.7512066653345022, grad=0.0, op='tanh')]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a binary classifier\n",
    "\n",
    "# Inputs: x training data\n",
    "xt = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0,  1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "# Desired outputs: binary classifier, either -1 or 1 for each feature (in this case, row) in the input dataset\n",
    "yt = [-1.0, 1.0, 1.0, -1.0]\n",
    "\n",
    "# Convert xt to a list of lists of ZeroDimTensor objects\n",
    "xt_tensors = [[ZeroDimTensor(val, label=f\"x_feature{j}\") for j, val in enumerate(row)] for row in xt]\n",
    "yt_tensors = [ZeroDimTensor(val, label=\"target\") for val in yt] \n",
    "\n",
    "# 1. Create the MultiLayerPerceptron model\n",
    "model = MultiLayerPerceptron(num_inputs=3, num_outputs_per_layer=[4, 1])\n",
    "\n",
    "yhats = [model(x_row) for x_row in xt_tensors] \n",
    "\n",
    "yhats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f19037-ba44-49cb-b61b-80d7ae18b4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9fff9-3768-418e-afe2-4a4796d629b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
