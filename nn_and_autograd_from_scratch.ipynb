{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f1edb4-daa9-41e3-8d73-221f08205539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b492a87b-b95b-4d6a-9650-c9c993a00b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroDimTensor:\n",
    "    def __init__(self, val, _children=(), _op=\"\", label=\"\"):\n",
    "        self.val = float(val)\n",
    "        self.grad = 0.0\n",
    "        self._op = _op # '_op' for consistency with Pytorch\n",
    "        self.label = label\n",
    "        self._children = set(_children)\n",
    "        self._backward = lambda: None # this i \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ZeroDimTensor(label='{self.label}', val={self.val}, grad={self.grad}, op='{self._op}')\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, ZeroDimTensor) else ZeroDimTensor(other)\n",
    "        return_val = ZeroDimTensor(self.val + other.val, (self, other), \"+\")\n",
    "\n",
    "        def _backward_pass_add():\n",
    "            # Gradients accumulate, so use += instead of = for all backward pass methods\n",
    "            self.grad += return_val.grad\n",
    "            other.grad += return_val.grad\n",
    "            \n",
    "        return_val._backward = _backward_pass_add # Assign to _backward\n",
    "        \n",
    "        return return_val\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, ZeroDimTensor) else ZeroDimTensor(other)\n",
    "        return_val = ZeroDimTensor(self.val * other.val, (self, other), \"*\")\n",
    "        \n",
    "        def _backward_pass_mul():\n",
    "            self.grad += other.val * return_val.grad\n",
    "            other.grad += self.val * return_val.grad\n",
    "            \n",
    "        return_val._backward = _backward_pass_mul # Assign to _backward\n",
    "        \n",
    "        return return_val\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, ZeroDimTensor) else ZeroDimTensor(other)\n",
    "        return self + (-other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"Power must be a scalar\"\n",
    "        return_val = ZeroDimTensor(self.val**other, (self,), f'**{other}')\n",
    "        \n",
    "        def _backward_pass_pow():\n",
    "            self.grad += return_val.grad * (other * (self.val**(other-1)))\n",
    "        return_val._backward = _backward_pass_pow\n",
    "        return return_val\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        # Ensure 'other' is a ZeroDimTensor\n",
    "        other = other if isinstance(other, ZeroDimTensor) else ZeroDimTensor(other)\n",
    "        # Forward pass for subtraction: A - B\n",
    "        return_val = ZeroDimTensor(self.val - other.val, (self, other), \"-\")\n",
    "\n",
    "        def _backward_pass_sub():\n",
    "            self.grad += return_val.grad * 1.0\n",
    "            other.grad += return_val.grad * -1.0 # Gradient with respect to subtracted term is negative\n",
    "            \n",
    "        return_val._backward = _backward_pass_sub\n",
    "        return return_val\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"Power must be a scalar\"\n",
    "        return_val = ZeroDimTensor(self.val**other, (self,), f'**{other}')\n",
    "        \n",
    "        def _backward_pass_pow():\n",
    "            self.grad += return_val.grad * (other * (self.val**(other-1)))\n",
    "        return_val._backward = _backward_pass_pow\n",
    "        return return_val\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        # Ensure 'other' is a ZeroDimTensor\n",
    "        other = other if isinstance(other, ZeroDimTensor) else ZeroDimTensor(other)\n",
    "        # Forward pass for division: A / B\n",
    "        return_val = ZeroDimTensor(self.val / other.val, (self, other), \"/\")\n",
    "\n",
    "        # If C = A / B, then dL/dA = dL/dC * (1/B)\n",
    "        # dL/dB = dL/dC * (-A/B^2)\n",
    "        def _backward_pass_div():\n",
    "            self.grad += return_val.grad * (1.0 / other.val)\n",
    "            other.grad += return_val.grad * (-self.val / (other.val**2))\n",
    "            \n",
    "        return_val._backward = _backward_pass_div\n",
    "        return return_val\n",
    "    \n",
    "    def tanh(self):\n",
    "        return_val = ZeroDimTensor(math.tanh(self.val), (self,), \"tanh\")\n",
    "        \n",
    "        def _backward_pass_tanh():\n",
    "            # Correct local derivative for tanh\n",
    "            self.grad += (1 - return_val.val**2) * return_val.grad \n",
    "            \n",
    "        return_val._backward = _backward_pass_tanh # Assign to _backward\n",
    "        \n",
    "        return return_val\n",
    "\n",
    "    def backward(self): # Implements the backward pass (like PyTorch API)\n",
    "        # Zero out all gradients in the graph before starting\n",
    "        nodes_to_zero = set()\n",
    "        stack = [self]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            if node not in nodes_to_zero:\n",
    "                nodes_to_zero.add(node)\n",
    "                for child in node._children:\n",
    "                    stack.append(child)\n",
    "        \n",
    "        for node in nodes_to_zero:\n",
    "            node.grad = 0.0\n",
    "\n",
    "        topological_ordering = []\n",
    "        visited = set()\n",
    "        \n",
    "        def topological_sort(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._children:\n",
    "                    topological_sort(child)\n",
    "                topological_ordering.append(node)\n",
    "\n",
    "        self.grad = 1.0 # Derivative of the root node (loss) is always 1\n",
    "        topological_sort(self)\n",
    "\n",
    "        for tensor in topological_ordering[::-1]: # Iterate in reverse topological order\n",
    "            tensor._backward() # Call the stored backward function\n",
    "\n",
    "        # For debugging: print tensors and their gradients after backward pass\n",
    "        # print(\"\\n--- Tensors and Gradients (after backward pass) ---\")\n",
    "        # for tensor in topological_ordering:\n",
    "        #     print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b70971-1c99-4020-91e0-c349490a5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = ZeroDimTensor(2.0, (), \"\", \"x1\")\n",
    "w1 = ZeroDimTensor(-3.0, (), \"\", \"w1\")\n",
    "\n",
    "x2 = ZeroDimTensor(0.0, (), \"\", \"x2\")\n",
    "w2 = ZeroDimTensor(1.0, (), \"\", \"w2\")\n",
    "\n",
    "b = ZeroDimTensor(6.8814, (), \"\", \"b\")\n",
    "\n",
    "w1x1 = w1 * x1\n",
    "w1x1.label = \"w1x1\"\n",
    "\n",
    "w2x2 = w2 * x2\n",
    "w2x2.label = \"w2x2\"\n",
    "\n",
    "w1x1_plus_w2x2 = w1x1 + w2x2\n",
    "w1x1_plus_w2x2.label = \"w1x1_plus_w2x2\"\n",
    "\n",
    "n = w1x1_plus_w2x2 + b\n",
    "n.label = \"n\"\n",
    "\n",
    "o = n.tanh()\n",
    "o.label = \"o\"\n",
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88cd3800-4a45-432f-8b7b-823d2541ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, num_inputs, nonlin=True):\n",
    "        # Initialize weights as a list of ZeroDimTensor objects\n",
    "        self.weights = [ZeroDimTensor(random.uniform(-1,1), label=f\"w{i}\") for i in range(num_inputs)] # random weights with labels\n",
    "        # Initialize bias as a ZeroDimTensor object\n",
    "        self.bias = ZeroDimTensor(random.uniform(-1,1), label=\"b\")\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.weights + [self.bias]\n",
    "\n",
    "    def __call__(self, x_inputs):\n",
    "        raw_activation = ZeroDimTensor(0.0, label=\"raw_act\") \n",
    "        \n",
    "        # Calculate w_i * x_i + b\n",
    "        for i in range(len(x_inputs)):\n",
    "            # Ensure x_inputs[i] is a ZeroDimTensor. If it's a float, __mul__ will convert it.\n",
    "            # But passing ZeroDimTensor explicitly is safer for graph consistency.\n",
    "            raw_activation += x_inputs[i] * self.weights[i]\n",
    "        \n",
    "        raw_activation += self.bias \n",
    "\n",
    "        # Apply the activation function (tanh) if nonlin == True\n",
    "        if (self.nonlin):\n",
    "            return raw_activation.tanh() # This proves that perceptrons and neural nets are just mathematical expressions\n",
    "        else:\n",
    "            return raw_activation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c9d48f-55cf-4587-b49b-6f88bc3bb1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZeroDimTensor(label='', val=-0.9923646169421559, grad=0.0, op='tanh')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = Perceptron(2)\n",
    "x = [9.0, 10.0]\n",
    "neuron(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff25e4c-45f7-4f2e-8a59-6b5034366c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_inputs, num_outputs, nonlin=True):\n",
    "        # Initialize the layer of perceptrons with random weights and biases\n",
    "        self.perceptrons = [Perceptron(num_inputs, nonlin=nonlin) for _ in range(num_outputs)] # Renamed for clarity\n",
    "\n",
    "    def parameters(self):\n",
    "        return [param for perceptron in self.perceptrons for param in perceptron.parameters()]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Do a forward pass, passing in x (inputs) for each perceptron in the layer\n",
    "        # The output is a list of ZeroDimTensor objects\n",
    "        return [perceptron(x) for perceptron in self.perceptrons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759d5da2-c64d-47e6-8cb1-49333a1ba7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ZeroDimTensor(label='', val=-0.9994436104411393, grad=0.0, op='tanh'),\n",
       " ZeroDimTensor(label='', val=-0.9999999997185669, grad=0.0, op='tanh'),\n",
       " ZeroDimTensor(label='', val=0.8107934154239987, grad=0.0, op='tanh')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Layer(2, 3) # 2 inputs per perceptron, and there is a layer of 3 perceptrons\n",
    "x = [9.0, 10.0]\n",
    "layer(x) # output the forward pass for 3 perceptrons each with input values x, random weights, and a random bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92cd27b-cf20-41fa-a14b-d3724a40f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, num_inputs, num_outputs_per_layer):\n",
    "        # sizes = [input_layer_size, hidden_layer1_size, ..., output_layer_size]\n",
    "        sizes = [num_inputs] + num_outputs_per_layer \n",
    "\n",
    "        # Create layers such that the number of outputs for each layer correctly matches the number of inputs for the next one\n",
    "        self.layers = []\n",
    "        for i in range(len(num_outputs_per_layer)): \n",
    "            # Check if this is the last layer\n",
    "            is_last_layer = (i == len(num_outputs_per_layer) - 1)\n",
    "            # Pass nonlin=False for the last layer to prevent vanishing gradient, since tanh will make gradient close to zero\n",
    "            self.layers.append(Layer(sizes[i], sizes[i+1], nonlin=not is_last_layer))\n",
    "\n",
    "    def parameters(self):\n",
    "        return [param for layer in self.layers for param in layer.parameters()]\n",
    "\n",
    "    def __call__(self, x_inputs):\n",
    "        # Pass the input through each layer sequentially\n",
    "        for layer in self.layers:\n",
    "            x_inputs = layer(x_inputs) # The output of one layer becomes the input for the next\n",
    "        return x_inputs # The final output of the network\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e4f7bbf-08fd-4dde-9f90-e19e069d39a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ZeroDimTensor(label='', val=-0.4672820014075061, grad=0.0, op='+')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1.0, 2.0, -3.0]\n",
    "nn = MultiLayerPerceptron(3, [4, 4, 1])\n",
    "nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c581fbe-90ff-486a-8461-2e6ef477e092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ZeroDimTensor(label='', val=1.1373464153052226, grad=0.0, op='+')],\n",
       " [ZeroDimTensor(label='', val=0.14212179811762576, grad=0.0, op='+')],\n",
       " [ZeroDimTensor(label='', val=2.119856705177905, grad=0.0, op='+')],\n",
       " [ZeroDimTensor(label='', val=0.9356174752091296, grad=0.0, op='+')]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a binary classifier\n",
    "\n",
    "# Inputs: x training data\n",
    "xt = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0,  1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "# Desired outputs: binary classifier, either -1 or 1 for each feature (in this case, row) in the input dataset\n",
    "yt = [-1.0, 1.0, 1.0, -1.0]\n",
    "\n",
    "# Convert into ZeroDimTensor objects\n",
    "xt_tensors = [[ZeroDimTensor(val, label=f\"x_feature{j}\") for j, val in enumerate(row)] for row in xt]\n",
    "yt_tensors = [ZeroDimTensor(val, label=\"target\") for val in yt] \n",
    "\n",
    "# 1. Create the MultiLayerPerceptron model\n",
    "model = MultiLayerPerceptron(num_inputs=3, num_outputs_per_layer=[4, 1])\n",
    "\n",
    "yhats = [model(x_row) for x_row in xt_tensors] \n",
    "\n",
    "yhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74f19037-ba44-49cb-b61b-80d7ae18b4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZeroDimTensor(label='', val=2.576224689687475, grad=0.0, op='/')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss = ZeroDimTensor(0.0, label=\"total_loss_sum\")\n",
    "\n",
    "for i, y_pred_list in enumerate(yhats):\n",
    "    y_pred = y_pred_list[0] # Extract the single ZeroDimTensor\n",
    "    y_true = yt_tensors[i] \n",
    "    loss_on_example = (y_pred - y_true)**2\n",
    "    total_loss += loss_on_example\n",
    "\n",
    "mse = total_loss / len(xt_tensors)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5d9fff9-3768-418e-afe2-4a4796d629b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total learnable parameters: 21\n"
     ]
    }
   ],
   "source": [
    "# Get all learnable parameters from the model\n",
    "params = model.parameters()\n",
    "print(f\"Total learnable parameters: {len(params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0129e5f4-5afb-48f8-aea8-75c729e419aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning rate (hyperparameter)\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ede9ac1-349f-48e6-b4ad-4fde7d2ce844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Loop ---\n",
      "Epoch 0: MSE = 1.757808\n",
      "  Sample 0 Pred (raw): 1.1373, (tanh): 0.8135, Target: -1\n",
      "Epoch 10: MSE = 0.213597\n",
      "  Sample 0 Pred (raw): -0.7924, (tanh): -0.6598, Target: -1\n",
      "Epoch 20: MSE = 0.068575\n",
      "  Sample 0 Pred (raw): -1.1629, (tanh): -0.8220, Target: -1\n",
      "Epoch 30: MSE = 0.037826\n",
      "  Sample 0 Pred (raw): -1.3541, (tanh): -0.8750, Target: -1\n",
      "Epoch 40: MSE = 0.025777\n",
      "  Sample 0 Pred (raw): -1.4746, (tanh): -0.9005, Target: -1\n",
      "Epoch 50: MSE = 0.019480\n",
      "  Sample 0 Pred (raw): -1.5612, (tanh): -0.9156, Target: -1\n",
      "Epoch 60: MSE = 0.015636\n",
      "  Sample 0 Pred (raw): -1.6282, (tanh): -0.9258, Target: -1\n",
      "Epoch 70: MSE = 0.013051\n",
      "  Sample 0 Pred (raw): -1.6828, (tanh): -0.9332, Target: -1\n",
      "Epoch 80: MSE = 0.011197\n",
      "  Sample 0 Pred (raw): -1.7286, (tanh): -0.9389, Target: -1\n",
      "Epoch 90: MSE = 0.009803\n",
      "  Sample 0 Pred (raw): -1.7680, (tanh): -0.9434, Target: -1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training Loop ---\")\n",
    "for epoch in range(100): \n",
    "    total_loss = ZeroDimTensor(0.0, label=\"total_loss\") \n",
    "\n",
    "    # Forward pass for the entire dataset (batch gradient descent style)\n",
    "    yhats = [model(x_row) for x_row in xt_tensors] \n",
    "    \n",
    "    # Calculate loss for each example and sum them up\n",
    "    for i, y_pred_list in enumerate(yhats):\n",
    "        # For binary classification, y_pred_list will be [ZeroDimTensor(output_value)]\n",
    "        y_pred = y_pred_list[0] \n",
    "        y_true = yt_tensors[i]\n",
    "        \n",
    "        # Use MSE. The model outputs logits, so apply tanh here.\n",
    "        prediction_val = y_pred.tanh() # Apply tanh here to get a prediction between -1 and 1\n",
    "        loss_on_example = (prediction_val - y_true)**2\n",
    "        total_loss += loss_on_example\n",
    "\n",
    "    mse = total_loss / len(xt_tensors)\n",
    "    \n",
    "    mse.backward()\n",
    "\n",
    "    # Update parameters using Gradient Descent\n",
    "    for p in params:\n",
    "        # Recall, if gradient is positive, it means increasing the weight will increase the error, and vice versa\n",
    "        # Thus, move in the opposite direction of the gradient.\n",
    "        p.val -= learning_rate * p.grad \n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: MSE = {mse.val:.6f}\")\n",
    "        # Print a sample prediction for inspection\n",
    "        print(f\"  Sample 0 Pred (raw): {yhats[0][0].val:.4f}, (tanh): {yhats[0][0].tanh().val:.4f}, Target: {yt[0]:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a47da0-189d-496c-ba2b-f7703f545b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
