{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05f1edb4-daa9-41e3-8d73-221f08205539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b492a87b-b95b-4d6a-9650-c9c993a00b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroDimTensor:\n",
    "    def __init__(self, val, _children=(), op=\"\", label=\"\"):\n",
    "        self.val = float(val)\n",
    "        self.grad = 0.0\n",
    "        self.op = op\n",
    "        self.label = label\n",
    "        self._children = set(_children)\n",
    "        self._apply_backward_pass = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ZeroDimTensor(label='{self.label}', val={self.val}, grad={self.grad}, op='{self.op}')\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, ZeroDimTensor) else ZeroDimTensor(other)\n",
    "        return_val = ZeroDimTensor(self.val + other.val, (self, other), \"+\") # forward pass\n",
    "\n",
    "        def _backward_pass_add():\n",
    "            # gradients accumulate, so use += instead of =\n",
    "            self.grad += return_val.grad\n",
    "            other.grad += return_val.grad\n",
    "            \n",
    "        return_val._apply_backward_pass = _backward_pass_add\n",
    "        \n",
    "        return return_val\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, ZeroDimTensor) else ZeroDimTensor(other)\n",
    "        return_val = ZeroDimTensor(self.val * other.val, (self, other), \"*\") # forward pass\n",
    "        \n",
    "        def _backward_pass_mul():\n",
    "            # gradients accumulate, so use += instead of =\n",
    "            self.grad += other.val  * return_val.grad\n",
    "            other.grad += self.val * return_val.grad\n",
    "            \n",
    "        return_val._apply_backward_pass = _backward_pass_mul;\n",
    "        \n",
    "        return return_val\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def tanh(self):\n",
    "        return_val = ZeroDimTensor(math.tanh(self.val), (self,), \"tanh\") # forward pass\n",
    "        \n",
    "        def _backward_pass_tanh():\n",
    "            # gradients accumulate, so use += instead of =\n",
    "            self.grad += (1 - return_val.val**2) * return_val.grad \n",
    "            \n",
    "        return_val._apply_backward_pass = _backward_pass_tanh;\n",
    "        \n",
    "        return return_val\n",
    "\n",
    "    def backward(self): # in the nature of the Pytorch API\n",
    "        topological_ordering = []\n",
    "        visited = set()\n",
    "        \n",
    "        def topological_sort(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._children:\n",
    "                    topological_sort(child)\n",
    "                topological_ordering.append(node)\n",
    "\n",
    "        self.grad = 1.0 # derivative of the first node is always 1\n",
    "        topological_sort(self)\n",
    "\n",
    "        for tensor in topological_ordering[::-1]:\n",
    "            tensor._apply_backward_pass()\n",
    "\n",
    "        for tensor in topological_ordering:\n",
    "            print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43b70971-1c99-4020-91e0-c349490a5faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroDimTensor(label='x2', val=0.0, grad=0.4999813233768232, op='')\n",
      "ZeroDimTensor(label='w2', val=1.0, grad=0.0, op='')\n",
      "ZeroDimTensor(label='w2x2', val=0.0, grad=0.4999813233768232, op='*')\n",
      "ZeroDimTensor(label='w1', val=-3.0, grad=0.9999626467536464, op='')\n",
      "ZeroDimTensor(label='x1', val=2.0, grad=-1.4999439701304698, op='')\n",
      "ZeroDimTensor(label='w1x1', val=-6.0, grad=0.4999813233768232, op='*')\n",
      "ZeroDimTensor(label='w1x1_plus_w2x2', val=-6.0, grad=0.4999813233768232, op='+')\n",
      "ZeroDimTensor(label='b', val=6.8814, grad=0.4999813233768232, op='')\n",
      "ZeroDimTensor(label='n', val=0.8814000000000002, grad=0.4999813233768232, op='+')\n",
      "ZeroDimTensor(label='o', val=0.7071199874301226, grad=1.0, op='tanh')\n"
     ]
    }
   ],
   "source": [
    "x1 = ZeroDimTensor(2.0, (), \"\", \"x1\")\n",
    "w1 = ZeroDimTensor(-3.0, (), \"\", \"w1\")\n",
    "\n",
    "x2 = ZeroDimTensor(0.0, (), \"\", \"x2\")\n",
    "w2 = ZeroDimTensor(1.0, (), \"\", \"w2\")\n",
    "\n",
    "b = ZeroDimTensor(6.8814, (), \"\", \"b\")\n",
    "\n",
    "w1x1 = w1 * x1\n",
    "w1x1.label = \"w1x1\"\n",
    "\n",
    "w2x2 = w2 * x2\n",
    "w2x2.label = \"w2x2\"\n",
    "\n",
    "w1x1_plus_w2x2 = w1x1 + w2x2\n",
    "w1x1_plus_w2x2.label = \"w1x1_plus_w2x2\"\n",
    "\n",
    "n = w1x1_plus_w2x2 + b\n",
    "n.label = \"n\"\n",
    "\n",
    "o = n.tanh()\n",
    "o.label = \"o\"\n",
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "88cd3800-4a45-432f-8b7b-823d2541ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, num_inputs):\n",
    "        self.weight = [ZeroDimTensor(random.uniform(-1,1), label=\"weight\") for _ in range(num_inputs)] # random weights\n",
    "        self.bias = ZeroDimTensor(random.uniform(-1,1), label=\"bias\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate w_i * x_i + b\n",
    "        raw_activation = 0.0\n",
    "        for i in range(len(x)):\n",
    "            raw_activation += x[i] * self.weight[i]\n",
    "        raw_activation += b\n",
    "\n",
    "        return raw_activation.tanh() # a perceptron is just a series of mathematical transformations represented as a tensor graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "67c9d48f-55cf-4587-b49b-6f88bc3bb1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZeroDimTensor(label='', val=0.9999999999999355, grad=0.0, op='tanh')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = Perceptron(2)\n",
    "x = [9.0, 10.0]\n",
    "neuron(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1ff25e4c-45f7-4f2e-8a59-6b5034366c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        # initialize the layer of perceptrons with random weights and biases\n",
    "        self.layer_of_perceptrons = [Perceptron(num_inputs) for _ in range(num_outputs)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # do a forward pass, passing in x (inputs) for each perceptron in the layer\n",
    "        return [perceptron(x) for perceptron in self.layer_of_perceptrons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "759d5da2-c64d-47e6-8cb1-49333a1ba7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ZeroDimTensor(label='', val=0.9999999992128678, grad=0.0, op='tanh'),\n",
       " ZeroDimTensor(label='', val=0.9999999996028481, grad=0.0, op='tanh'),\n",
       " ZeroDimTensor(label='', val=0.9999999999999996, grad=0.0, op='tanh')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Layer(2, 3) # 2 inputs per perceptron, and there is a layer of 3 perceptrons\n",
    "x = [9.0, 10.0]\n",
    "layer(x) # output the forward pass for 3 perceptrons each with input values x, random weights, and a random bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d92cd27b-cf20-41fa-a14b-d3724a40f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        sizes = [num_inputs] + num_outputs # list concatenation. The ls will have all the layers\n",
    "        # sizes = [no perceptrons in input layer, num perceptrons in hidden layer 1, \n",
    "        #          num perceptrons in hidden layer size 2, ... , num perceptrons in the output layer] \n",
    "\n",
    "        # create layers such that the number of outputs for each layer correctly matches the number of inputs for the next one\n",
    "        self.layers = [Layer(sizes[i - 1], sizes[i]) for i in range(1, len(sizes))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) # compute forward pass at each layer, passing the output array as the input for the next layer\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f7bbf-08fd-4dde-9f90-e19e069d39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.0, 2.0, -3.0]\n",
    "nn = MultiLayerPerceptron(3, [4, 4, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
